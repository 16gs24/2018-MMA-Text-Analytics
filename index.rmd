---
title: "The Toronto Analytics Job Landscape"
author: "Summerhill"
date: '`r Sys.Date()`'
output:
  md_document:
    variant: markdown_github
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.width=10, fig.height=10, fig.path='Figs/',fig.align="center",
                      warning=FALSE,message=FALSE,error=FALSE,include=TRUE,echo=FALSE,
                      knitr.table.format="html")

set.seed(42)

# Mel - A note on code chunk options:
# include=TRUE means that output of the code is shown in the document (intended for graphics).
# echo=TRUE means the code is shown in the document (intended for code that might be interesting for a reader).
# message and warning are for the text for loading libraries or if a function fails
# echo=TRUE means the code is not shown in the final document
# http://kbroman.org/knitr_knutshell/pages/Rmarkdown.html
# http://rmarkdown.rstudio.com/developer_parameterized_reports.html
# https://guides.github.com/features/mastering-markdown/

```
- This document was rendered last on `r Sys.Date()`

##*THIS PROJECT IS STILL UNDER CONSTRUCTION*
The intention will be to mask the code as the project approaches completion.


## Authors
- To shower praise for ingenuity on the project, contact [Melody Liu](https://www.linkedin.com/in/meifei-melody-liu/)
- For criticism of avenues we couldn't investigate in 4 weeks contact [Gage Sonntag](https://www.linkedin.com/in/gage-sonntag/)

##Executive Summary
- This project was produced for the Text Analytics Workshop for the Winter 2018 Masters of Management Analytics Cohort at Queen's University
- The goal from the outset was to use text analytics techniques developed in class to examine jobs companies have posted on Indeed in Toronto
and employ techniques discussed in class including document clustering, topic modelling, and visualization.

##Project Rationale
- A open sourced project working with real world data was desired
- Other projects can be found scraping DS/Analytics jobs from Indeed. Typically word frequencies for keywords like Python or Hadoop are calculated
- Moving beyond that, we were interested in topic modelling and how the choice of words signals relationships between roles
- Job postings fit the 'bag of words' or ngram approach taught in class. Not many employers say **"We don't want someone who knows Python"**

```{r Import Libraries}
library(feather)
library(tidyverse)
library(tidytext)
library(tm)
library(wordcloud)
library(widyr)
library(ggraph)
library(igraph)
library(knitr)
library(ggridges)
library(RTextTools)
library(dendextend)
library(ggdendro)
library(clValid)
```

##Gathering Data
- Beautiful Soup & Selenium were used in Python to access [Indeed](https://www.indeed.ca/jobs?q=analytics&l=Toronto&start=10 "Indeed:Analytics Jobs in Toronto") and scrape unsponsored job titles, companies, and postings
- 1800 jobs were scraped from 9 search terms we believed captured the jobs most MMA students are pursuing.
- Jobs were passed from Python to R using [Feather](https://blog.rstudio.com/2016/03/29/feather/ "Feather: A Fast On-Disk Format for Data Frames for R and Python, powered by Apache Arrow")

```{r Import Data,echo=TRUE}
rm(list=ls())

#list our data files
searches <- c("analytics",
                 "data analyst",
                 "data scientist",
                 "analytics strategy",
                 "data insights",
                 "marketing analytics",
                 "analytics reporting",
                 "machine learning",
                 "business intelligence")

files <- paste("data/feather/",searches,".feather",sep="")

#read and collapse to data frame
datalist <- lapply(as.list(files),function(x){read_feather(x)})
test <- datalist[[1]]
data <- bind_rows(datalist,.id="search")
rm(datalist)

#fix quotations in column names
names(data) <- c("search","company","text","titles","urls")
data <- data %>% select(company,titles,text,search,urls)

#examine the uniqueness of our data
NumJobs <- n_distinct(data$urls)

#reduce to distinct jobs and clean up search column
data <- data[!duplicated(data$urls),]
data$search <- plyr::mapvalues(data$search,
                               from=unique(data$search),
                               to=searches)
```
- Our data returned `r NumJobs` unique jobs within our search.
- It's clear a considerable amount of cleaning is in order 

```{r preprocessing}
RemovePattern <- function(vector,pattern){gsub(pattern=pattern,replacement=" ",vector)}

data <- dmap(data,RemovePattern,"\n")
data <- dmap(data,RemovePattern,"\\(")   
data <- dmap(data,RemovePattern,"\\)")
data <- dmap(data,RemovePattern,"\\{[a-zA-Z0-9]\\}")
data$text <- RemovePattern(data$text,"[[:digit:]]")
```

```{r ggplot theme}
#to make defaults better for slide deck 
presentation <- theme_minimal()+
     theme(axis.text.x = element_text(size=12),
                      axis.text.y = element_text(size=12),
                      axis.title.x = element_text(size=14),
                      axis.title.y = element_text(size=14),
                      plot.title = element_text(size=16))

queens_colors <- scale_fill_manual(values=c("#11335d","#9d1939","#eebd31"))

```

```{r Jobs Found}
#investigate redundant jobs. Should return 200/each if they are all unique.

data %>%
     group_by(search) %>%
     summarize(NumberUniquePostings=n()) %>%
     ggplot(aes(x=factor(search,searches),y=NumberUniquePostings,fill="#11335d"))+geom_col() +
     labs(y="Number of Unseen Postings",
          title="Unique Postings by Sequential Search",
          x="Job Title") +
     presentation+
     queens_colors+
     theme(axis.text.x = element_text(angle = 30, hjust = 1))+
     theme(legend.position="none")
```

- We expect 200 jobs for each result, and removing the duplicate jobs in the order they were searched.
- Interestingly, searching 200 jobs in analytics returns only 113 unique jobs, some redundancy exists.
- As we search overlapping terms, data sciencist, data insights, fewer and fewer unique jobs are returned
- Interestingly, each additional search term returns a surprising amount of new jobs. 75 jobs are shown for machine learning that were not found for data scientist, a fairly similar field.
- Business Intelligence seems to be fairly lateral to other search terms, returning many unique jobs

```{r Job title frequency}
titles_to_use <- data %>%
     count(titles,sort=TRUE) %>%
     head(7) %>%
     mutate(titles=factor(titles,titles))

titles_plot <- data %>% 
     filter(titles %in% titles_to_use$titles) %>%
     count(titles,search,sort=TRUE) %>%
     mutate(frequency = n/NumJobs) %>%
     ungroup() %>%
     mutate(titles=factor(titles,titles_to_use$titles)) %>%
     mutate(search=factor(search,searches))

titles_order <- data %>% 
     count(titles,sort=TRUE) %>%
     filter(titles %in% titles_plot$titles)

titles_plot %>%
     ggplot(aes(x=titles,y=frequency,fill=search))+geom_col() + 
     scale_y_continuous(name="Frequency in Dataset",labels= scales::percent) + 
     labs(title="Most Frequent Job Titles",x="Job Title") + coord_flip() + 
     presentation+scale_fill_brewer(palette="Reds",direction=-1)
```


- The job search is currently dominated by data scientists, which have become a catch all word. But it's encouraging to see data engineering & machine learning engineering to begin to take hold.
- Analytics is surprisingly absent, but is likely wrapped into titles like "Manager, Analytics" which is more heterogeneous. Let's take a closer look at where our Analytics jobs are.

```{r titles for analytics only}
analytics_jobs <- data %>% filter(search=="analytics") %>% n_distinct()

analytics_titles_plot <- data %>% 
     filter(search=="analytics") %>%
     count(titles,sort=TRUE) %>%
     head(7) %>%
     mutate(frequency = n/analytics_jobs)

analytics_titles_plot %>%
     ggplot(aes(x=reorder(titles,-frequency),y=frequency,fill="red"))+geom_col() + 
     theme(axis.text.x = element_text(angle = 30, hjust = 1)) + 
     scale_y_continuous(name="Frequency in Analytics Search",labels= scales::percent) +
     labs(title="Most Frequent Hiring Companies for Analytics",x="Company") + 
     coord_flip() +
     theme_minimal() + 
     theme(legend.position="none") + queens_colors
```

- Here we can see much more heterogeneity in the job titles used by Analytics Practioners vs Data Scientists.

```{r frequent companies}
company_plot <- data %>% 
     count(company,sort=TRUE) %>%
     mutate(frequency = n/NumJobs) %>%  
     mutate(company = factor(company,company)) %>%
     top_n(10)

company_plot %>%
     ggplot(aes(x=company,y=frequency,fill="red"))+geom_col() +
     presentation+
     queens_colors+
     theme(axis.text.x = element_text(angle = 30, hjust = 1))+
     scale_y_continuous(name="Frequency in Dataset",labels= scales::percent) + 
     labs(title="Most Frequent Hiring Companies",x="Company")+
     theme(legend.position="none")

```


- This seems to resonate with what the Toronto Job environment is as a whole: Telecom, Banking and consultancies.

# A Word Frequency Approach

```{r remove outliers}
MinWords <- 300

empty_urls <- data %>%
     unnest_tokens(token="words",output="unigrams",input=text) %>%
     group_by(urls) %>%
     count(urls,sort=TRUE) %>%
     filter(n < MinWords)

data <- data %>% 
     filter(!urls %in% empty_urls$urls)

NumJobs <- n_distinct(data$urls)
     
```

- The boiler plate at the end of each job posting, encouraging people to apply, discussing company acolades and culture distort our analysis. Let's spend some time cleaning up *job specific words* and *html related language*

```{r Process unigrams Data}
#what words to avoid
stop <- read.csv("stopwords.csv",stringsAsFactors = FALSE)
stop <- rbind(stop,
              data.frame(words=stopwords("en")))

#process n-grams
data %>%
     unnest_tokens(token="words",output="unigrams",input=text) %>%
     group_by(search,unigrams) %>%
     filter(!unigrams %in% stop$words) %>%
     count(search,unigrams,sort=TRUE) %>%
     top_n(4) %>%
     ungroup() %>%
     ggplot(aes(x=unigrams,y=n,fill=search))+
     geom_bar(stat="identity")+
     coord_flip()+
     presentation+scale_fill_brewer(palette="Reds",direction=-1)
```

- We are starting to look better. Let's take a look at our bigrams.
```{r Process bigrams}
#look a bi-grams
data %>%
     unnest_tokens(token="ngrams",n=2,output="tokens",input=text) %>%
     group_by(search,tokens) %>%
     filter(!tokens %in% searches) %>%
     separate(col=tokens,into=c("word1","word2"),sep=" ") %>%
     filter(!word1 %in% stop$words, !word2 %in% stop$words) %>%
     unite(tokens,word1,word2,sep=" ") %>%
     filter(!tokens %in% searches) %>%
     count(search,tokens,sort=TRUE) %>%
     top_n(3) %>%
     ungroup() %>%
     ggplot(aes(x=tokens,y=n,fill=search))+
     geom_bar(stat="identity")+
     coord_flip()+
     presentation+scale_fill_brewer(palette="Reds",direction=-1)
```
- This is less fruitful. Likely some bi-grams have value that are less frequent. Words like **machine learning** or **project managment**. They are likely mentioned once in a few job postings, but have a low count.
- We could cluster on tf-idf, but instead, let's first look at how often phrases are mentioned distinctly in jobs. This weights phrases mentioned in lots of jobs, not phrases mentioned many times.

```{r Process bigrams}
#look a bi-grams
data %>%
     unnest_tokens(token="ngrams",n=2,output="tokens",input=text) %>%
     group_by(search,tokens) %>%
     filter(!tokens %in% searches) %>%
     separate(col=tokens,into=c("word1","word2"),sep=" ") %>%
     filter(!word1 %in% stop$words, !word2 %in% stop$words) %>%
     unite(tokens,word1,word2,sep=" ") %>%
     ungroup() %>%
     distinct() %>%
     count(search,tokens,sort=TRUE) %>%
     top_n(3) %>%
     ggplot(aes(x=tokens,y=n,fill=search))+
     geom_bar(stat="identity")+
     coord_flip()+
     presentation+scale_fill_brewer(palette="Reds",direction=-1)

```
- This begins to get a bit more accurate of a assessment of what employers mention. Some of these are representative of the core requirements in analytics & DS: the fine line between communication and computer science, decision making & project management.  

- Typically when you see projects like this done, people look for some analytics or Data Science skills, and count the occurences.  We want to go beyond that, but lets examine the landscape for analytical skills in Toronto.

#A Skills Based Approach
```{r skills mentioned}

skills <- read_csv("skills.csv")
names(skills) <- c("tokens","domain")

data %>%
     unnest_tokens(token="words",output="tokens",input=text) %>%
     filter(tokens %in% skills$tokens) %>%
     group_by(tokens) %>%
     count(tokens,sort=TRUE) %>%
     top_n(12) %>%
     left_join(skills) %>%
     mutate(tokens=factor(tokens,tokens)) %>%
     ggplot(aes(x=tokens,y=n,fill=domain)) +
     geom_col() + 
     labs(title="Skills Present in Dataset",x="Skill",y="Number of Occurences in Dataset")+
     theme_minimal()+
     theme(axis.text.x = element_text(angle = 30, hjust = 1),
           plot.title = element_text(hjust = 0.5)) + 
     queens_colors
     

     
```
- This seems to suggest excel, R and SQL are in high demand. Let's examine how inter related these concepts are.
- Are the same jobs looking for R excel and SQL?
- How many of these skills are required for different jobs?

```{r frequency of skills}

data %>%
     unnest_tokens(token="words",output="tokens",input=text) %>%
     filter(tokens %in% skills$tokens) %>%
     select(search,tokens,urls) %>%
     distinct() %>%
     group_by(search,urls) %>%
     count(urls,sort=TRUE) %>%
     ggplot(aes(x=n,y=search,fill=search))+geom_density_ridges()+
     labs(title="Mentions of Skills in Job Postings",x="Number of Skills",y="Search Term")+
     scale_x_continuous(limits=c(0,8)) +
     theme_minimal()+
     theme(plot.title = element_text(hjust = 0.5)) +
     scale_fill_cyclical(values=c("#11335d","#9d1939","#eebd31"))
```
- For the skills we have selected, analytics and data scientists have long tails. These are likely associated with the similarity between the big data tools we selected: hive, scala, spark etc.
- Let's see how theses jobs get mentioned together. 

# A Network Diagram of Skills
```{r pairwise correlation}
#pairwise correlation
data %>%
     unnest_tokens(token="words",output="tokens",input=text) %>%
     filter(tokens %in% skills$tokens) %>%
     pairwise_cor(tokens,urls,sort=TRUE) %>%
     filter(correlation > .18) %>%
     graph_from_data_frame() %>%
     ggraph(layout = "fr") +
     geom_edge_link(aes(#edge_width=correlation,
                         edge_alpha = correlation),show.legend = FALSE) +
     geom_node_point(color = "#eebd31", size = 8) +
     geom_node_text(aes(label = name), repel = TRUE) +
     theme_void()

```
- The network analysis shown shows a few unique clusters. Excel and powerpoint don't seem correlated with the rest of our tech stack, despite the frequent mentions of excel (which presumably are the noun and not the verb)
- 3 clusters seem present:
     - Traditional Analytics - R, SAS, and a smal relationship to
     - Big Data - Python leveraging Hadoop, AWS, Scala and spark
     - BI/Data Viz - Tableau, SQL and qlik
- Our Trifecta of R, SQL, and excel don't seem as complimentary skills anymore

- Let's look at clustering our data set, to see if these groups are also represented

#Clustering

```{r clustering - Scree Plot}

#Frequency filters

minFreq = 0.05
maxFreq = 0.80

#creating identifier
data$ID <- paste(data$company,data$titles,sep="_")

#filtering out stopwords and infrequent/frequent words from unigrams
clean_unigrams <- data %>%
     unnest_tokens(token="words",output="tokens",input=text) %>%
     select(urls,tokens) %>%
     distinct() %>%
     filter(!tokens %in% stop$words) %>%
     mutate(tokens=wordStem(tokens)) %>%
     group_by(tokens) %>%
     count(tokens,sort=TRUE) %>%
     mutate(frequency=n/NumJobs) %>%
     filter(frequency> minFreq & frequency< maxFreq)

#creating clean unigrams DTM
clustering_unigrams <- data %>%
     unnest_tokens(token="words",output="tokens",input=text) %>%
     select(ID,tokens) %>%
     mutate(tokens=wordStem(tokens)) %>%
     filter(tokens %in% clean_unigrams$tokens) %>%
     count(ID,tokens,sort=TRUE) %>%
     ungroup() %>%
     spread(tokens,n,fill=0) 

#filtering out stopwords and infrequent/frequent words from bigrams
clean_bigrams <- data %>%
     unnest_tokens(token="ngrams",n=2,output="tokens",input=text) %>%
     select(urls,tokens) %>%
     distinct() %>%
     separate(col=tokens,into=c("word1","word2"),sep=" ") %>%
     filter(!word1 %in% stop$words, !word2 %in% stop$words) %>%
     mutate(word1=wordStem(word1)) %>%
     mutate(word2=wordStem(word2)) %>%
     unite(tokens,word1,word2,sep=" ") %>%
     group_by(tokens) %>%
     count(tokens,sort=TRUE) %>%
     ungroup() %>%
     mutate(frequency=n/NumJobs) %>%
     filter(frequency> minFreq & frequency< maxFreq)

#creating clean bigrams dataset
clustering_bigrams <- data %>%
     unnest_tokens(token="ngrams",n=2,output="tokens",input=text) %>%
     select(ID,tokens) %>%
     separate(col=tokens,into=c("word1","word2"),sep=" ") %>%
     mutate(word1=wordStem(word1)) %>%
     mutate(word2=wordStem(word2)) %>%
     unite(tokens,word1,word2,sep=" ") %>%
     filter(tokens %in% clean_bigrams$tokens) %>% 
     count(ID,tokens,sort=TRUE) %>%
     ungroup() %>%
     spread(tokens,n,fill=0) 

#Joining unigrams and bigrams cluster data together, replace NA's
clustering_data <-  clustering_unigrams %>%
     full_join(clustering_bigrams) %>%
     slice(-c(58,111,241,328,435,529))

#collapse to numeric
clustering_data[is.na(clustering_data)] <- 0
clusteringID <- clustering_data$ID
clustering_data <- select(clustering_data,-ID)
     
```

- An initial pass using hierarchical clustering revealed a number of outlier jobs, which were removed from the data set. The work will not be shown here, for brevity's sake. After removing these, let's look at how K-means clustering performs.

```{r}
#accumulator of clustering results
clust_results <- data.frame()

#run kmeans for all clusters up to 30
for(i in 1:30) {
     k_clust <- kmeans(clustering_data, centers=i, iter.max =100)
     #Combine cluster number and cluster together, write to df
     clust_results <- rbind(clust_results,cbind(i,k_clust$tot.withinss))
}
names(clust_results) <- c("cluster","results")

#scree elbow plot
ggplot(clust_results,aes(x=cluster,y=results))+
geom_point(col="#9d1939",size=4)+geom_line(col="#9d1939",size=2)+
     theme_minimal()+
     presentation +
     theme(axis.text.x = element_text(angle = 30)) +
     labs(title="Scree Plot - Within Cluster Variance vs Number of clusters",
          x="Number of Clusters",
          y="Within Cluster Sum of Squares")

validation_metrics <- clValid(as.matrix(clustering_data),3:10,clMethods="kmeans",
                              validation="internal")
```
- Plotting the within cluster sum of squares vs number of clusters produces a scree plot. Here, good clustering could be judged by the slope of the line decreasing rapidly after the ideal clustering was run.  here this is not the case, with a shallow change in slope.
- Evaluating instead by Dunn's Metric, which judgues clusters by the means of clusters, the distance between clusters and the within cluster variance.  Here, we find the ideal cluster size to be 7. Let's dive a litle further into our clustering results.

```{r 7 cluster performance}
kmeans7 <- kmeans(clustering_data,centers=7,nstart=100)
cluster7results <- data.frame(ID=as.character(clusteringID),clusters=kmeans7$cluster)
data <- data %>%
     mutate(ID=paste(company,titles,sep="_"))

left_join(cluster7results,data,by="ID") %>%
     select(search,clusters) %>%
     count(search,clusters,sort=TRUE) %>%
     ggplot(aes(x=search,y=clusters,fill=n))+
     geom_tile()+
     geom_text(aes(label=n))+
     presentation+
     scale_fill_gradient(low="#11335d",high="#9d1939")+
     theme(axis.text.x = element_text(angle = 30, hjust = 1),
           plot.title = element_text(hjust = 0.5)) + 
     labs(title="Density of Cluster Assignment vs Initial Search Term",
          x="Search Term",
          y="Cluster Number")
     
```

- While it seemed at first glance there is some structure measured from the clustering, cluster 2 may represent some of the less technical roles in data analysis and BI, and cluster 1 has a notable amount of DS & ML jobs, the bulk of the data is sucked up in Cluster 5, and the rest are selected as outliers.

- In reality, these 7 clusters are really just 3. 

- K-means is sensitive to multi-dimensional outliers, which are hard to identify. With more work identifiying them and filtering them out, we could achieve more resolution between our clusters.


