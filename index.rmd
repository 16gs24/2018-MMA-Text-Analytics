---
title: "The Toronto Analytics Job Landscape"
author: "Summerhill"
date: "`r Sys.Date()`"
output:
  md_document:
    variant: markdown_github
  html_document: default
  pdf_document: default
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.width=10, fig.height=10, fig.path='Figs/',
                      warning=FALSE,message=FALSE,error=FALSE,include=TRUE,echo=TRUE)

set.seed(42)

# a note on options: 
# include=TRUE means that output of the code is shown in the document (intended for graphics).
# echo=TRUE means the code is shown in the document (intended for code that might be interesting for a reader).
# message and warning are for the text for loading libraries or if a function fails
# echo=TRUE means the code is not shown in the final document
# http://kbroman.org/knitr_knutshell/pages/Rmarkdown.html
# http://rmarkdown.rstudio.com/developer_parameterized_reports.html
# https://guides.github.com/features/mastering-markdown/

```
- This document was rendered last on `r Sys.Date()`

##*THIS PROJECT IS STILL UNDER CONSTRUCTION*
The intention will be to mask the code as the project approaches completion.

##Executive Summary
- This project was produced for the Text Analytics Workshop for the Winter 2018 Masters of Management Analytics Cohort at Queen's University
- The goal from the outset was to use text analytics techniques developed in class to examine jobs companies have posted on Indeed in Toronto
and employ techniques discussed in class including document clustering, topic modelling, and visualization.

##Project Rationale
- A open sourced project working with real world data was desired
- Other projects can be found scraping DS/Analytics jobs from Indeed. Typically word frequencies for keywords like Python or Hadoop are calculated
- Moving beyond that, we were interested in topic modelling and how the choice of words signals relationships between roles
- Job postings fit the 'bag of words' or ngram approach taught in class. Not many employers say **"We don't want someone who knows Python"**

```{r Import Libraries}
library(feather)
library(tidyverse)
library(tidytext)
library(tm)
library(wordcloud)
library(widyr)
library(ggraph)
library(igraph)
```

##Gathering Data
- Beautiful Soup & Selenium were used in Python to access [Indeed](https://www.indeed.ca/jobs?q=analytics&l=Toronto&start=10 "Indeed:Analytics Jobs in Toronto") and scrape unsponsored job titles, companies, and postings
- `later number` unique jobs were scraped from the search terms: `analytics`,`etc`....
- Jobs were passed from Python to R using [Feather](https://blog.rstudio.com/2016/03/29/feather/ "Feather: A Fast On-Disk Format for Data Frames for R and Python, powered by Apache Arrow")

```{r Import Data,echo=TRUE}
rm(list=ls())
#list our data files
searches <- c("analytics",
                 "data analyst",
                 "data scientist",
                 "analytics strategy",
                 "data insights",
                 "marketing analytics",
                 "analytics reporting",
                 "machine learning",
                 "business intelligence")

files <- paste("data/feather/",searches,".feather",sep="")

#read and collapse to data frame
datalist <- lapply(as.list(files),function(x){read_feather(x)})
test <- datalist[[1]]
data <- bind_rows(datalist,.id="search")
rm(datalist)

#fix quotations in column names
names(data) <- c("search","company","text","titles","urls")

#check if we have redundant jobs
sum(duplicated(data[,2:4]))

#examine the uniqueness of our data
#lapply(data,function(x){length(unique(x))})
NumJobs <- length(unique(data$urls))

#reduce to distinct jobs and clean up search column
data <- data[!duplicated(data$urls),]
data$search <- plyr::mapvalues(data$search,
                               from=unique(data$search),
                               to=searches)

head(data)
```
```{r preprocessing}
RemovePattern <- function(vector,pattern){gsub(pattern=pattern,replacement="",vector)}
#data <- map(data,RemovePattern,"\n")

```

```{r Jobs Found}
#investigate redundant jobs. Should return 200/each if they are all unique.

rollup <- data %>%
     group_by(search) %>%
     summarize(NumberUniquePostings=n()) 

#sort by search order
left_join(data.frame(search=searches),rollup,by="search")

```

- We expect 200 jobs for each result, and removing the duplicate jobs in the order they were searched.
- Interestingly, searching 200 jobs in analytics returns only 113 unique jobs, some redundancy exists.
- As we search overlapping terms, data sciencist, data insights, fewer and fewer unique jobs are returned
- Interestingly, each additional search term returns a surprising amount of new jobs. 75 jobs are shown for machine learning that were not found for data scientist, a fairly similar field.
- Business Intelligence seems to be fairly lateral to other search terms, returning many unique jobs



```{r Process unigrams Data}

#what words to avoid
stops <- stopwords("en")

#process n-grams
unigrams <- data %>%
     unnest_tokens(token="words",output="unigrams",input=text) %>%
     group_by(unigrams) %>%
     filter(!unigrams %in% stops) %>%
     count(unigrams,sort=TRUE)

#visualize
wordcloud(unigrams$unigrams,unigrams$n,max.words=100)

```

- Looking at a simple word frequency, we see out of the box our data is very messy
- The boiler plate at the end of each job posting, encouraging people to apply, discussing company acolades and culture distort our analysis. Let's spend some time cleaning up 0-value words.



```{r Process bigrams}
#look a bi-grams
bigrams_totals <- data %>%
     unnest_tokens(token="ngrams",n=2,output="tokens",input=text) %>%
     separate(col=tokens,into=c("word1","word2"),sep=" ") %>%
     filter(!word1 %in% stops, !word2 %in% stops) %>%
     unite(tokens,word1,word2,sep=" ") %>%
     count(tokens,sort=TRUE)

head(bigrams_totals,20)

wordcloud(bigrams_totals$tokens,bigrams_totals$n,max.words=20)
```



```{r Unigram frequency}

#determine how frequent each word occurs across job postings. Don't skip stop words yet.
unigrams_freq <- data %>%
     unnest_tokens(token="words",output="tokens",input=text) %>%
     select(urls,tokens) %>%
     distinct() %>%
     group_by(tokens) %>%
     count(tokens,sort=TRUE) %>%
     ungroup() %>%
     mutate(frequency=n/NumJobs)

tail(unigrams_freq,20)

#determine how frequent each bigram is across job postings. Don't skip stop words yet.
bigrams_freq <- data %>%
     unnest_tokens(token="ngrams",n=2,output="tokens",input=text) %>%
     select(urls,tokens) %>%
     distinct() %>%
     group_by(tokens) %>%
     count(tokens,sort=TRUE) %>%
     ungroup() %>%
     mutate(frequency=n/NumJobs)

head(bigrams_freq,20)

```



```{r pairwise count}

skills <- c("sas","python","excel","powerpoint","sql",
                          "r","hadoop","spark","java","scala","aws",
                          "tableau","microstrategy","spss","c++")

data %>%
     unnest_tokens(token="words",output="tokens",input=text) %>%
     filter(tokens %in% skills) %>%
     pairwise_count(tokens,urls,sort=TRUE)
```





```{r pairwise correlation}
#pairwise correlation
data %>%
     unnest_tokens(token="words",output="tokens",input=text) %>%
     filter(tokens %in% skills) %>%
     pairwise_cor(tokens,urls,sort=TRUE) %>%
     filter(correlation > .1) %>%
     graph_from_data_frame() %>%
     ggraph(layout = "fr") +
     geom_edge_link(aes(edge_alpha = correlation), show.legend = FALSE) +
     geom_node_point(color = "lightblue", size = 5) +
     geom_node_text(aes(label = name), repel = TRUE) +
     theme_void()

```

```{r clustering}

#creating tokens - once we come up with list of stop words, should use anti_join here to remove them

data_token <- data %>%
     unnest_tokens(word,text)

#creating unique document identifiers for each job posting 
data_token$ID <- paste(data_token$company,data_token$titles,sep="_")

#removing useless columns - 
data_token <- data_token[,-(1:4)]
head(data_token)

#frequency of words by document

data_count <- data_token %>%
     count(ID,word,sort=TRUE)

#creating table of total count of words in each document 
data_count2 <- data_count %>%
     group_by(ID) %>%
     summarise(total = sum(n))

#joining the total count information with primary dataset
data_count <- right_join(data_count,data_count2)

#creating the correlation column so we can filter out words <1% or >80% of docs 
data_count <- data_count %>%
     mutate(correlation = n/total) %>%
     arrange(desc(correlation))

#Note for Gage: I'm noticing something odd for some of these postings. Look at the head of the data frame. The Brain Hunter Systems job posting only has 2 words but when I go to the actual posting itself, it clearly has way more than that 

head(data_count)


#Correlation filters
minCor = 0.01
maxCor = 0.80

#filter out words <1% or >80% of docs 
data_count <- data_count %>%
     filter(correlation > minCor & correlation < maxCor)

#casting tidy text data into DTM
data_dtm <- data_count %>%
     cast_dtm(ID,word,n)

#looking at most popular words, min freq of 20
findFreqTerms(data_dtm,lowfreq=20)

#accumulator of clustering results 
clust_results <- data.frame()

#run kmeans for all clusters up to 15
set.seed(50)
for(i in 1:15) {
     k_clust <- kmeans(data_dtm, centers=i, iter.max =100)
     #Combine cluster number and cluster together, write to df
     clust_results <- rbind(clust_results,cbind(i,k_clust$tot.withinss))
}
names(clust_results) <- c("cluster","results")

#elbow plot
ggplot(clust_results,aes(x=cluster,y=results,group=1))+
geom_jitter()+geom_line()
## above plot shows that at about 12 clusters, any additional clusters would lose significance

#Let's look at 8 clusters: need to figure out how to PLOT the clusters here
kmeans8 <- kmeans(data_dtm,centers=8)


###hierarchical clustering

#convert to TDM
data_tdm <- t(data_dtm)

#reducing sparsity 
dim(data_tdm)
data_clean <- removeSparseTerms(data_tdm,sparse = 0.80)
data_m <- as.matrix(data_clean)
data_df <- as.data.frame(data_m)

#calculate distance
data_dist <- dist(data_df,method="euclidean")
hc <- hclust(data_dist)
plot(hc)
```





