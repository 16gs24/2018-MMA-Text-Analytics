---
title: "The Toronto Analytics Job Landscape"
author: "Summerhill"
date: '`r Sys.Date()`'
output:
  md_document:
    variant: markdown_github
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.width=10, fig.height=10, fig.path='Figs/',fig.align="center",
                      warning=FALSE,message=FALSE,error=FALSE,include=TRUE,echo=FALSE,
                      knitr.table.format="html")

set.seed(42)

# Mel - A note on code chunk options:
# include=TRUE means that output of the code is shown in the document (intended for graphics).
# echo=TRUE means the code is shown in the document (intended for code that might be interesting for a reader).
# message and warning are for the text for loading libraries or if a function fails
# echo=TRUE means the code is not shown in the final document
# http://kbroman.org/knitr_knutshell/pages/Rmarkdown.html
# http://rmarkdown.rstudio.com/developer_parameterized_reports.html
# https://guides.github.com/features/mastering-markdown/

```

- This document was rendered last on `r Sys.Date()`

##Authors
- To shower praise for ingenuity on the project, contact [Melody Liu](https://www.linkedin.com/in/meifei-melody-liu/)
- For criticism of avenues we couldn't investigate in 4 weeks contact [Gage Sonntag](https://www.linkedin.com/in/gage-sonntag/)

##Executive Summary
- This project was produced for the Text Analytics Workshop for the Winter 2018 Masters of Management Analytics Cohort at Queen's University
- The goal from the outset was to use text analytics techniques developed in class to examine jobs companies have posted on Indeed in Toronto
and employ techniques discussed in class including some of: tokenization, data cleaning, document clustering, topic modelling, network analysis and visualization.

##Project Rationale
- A open sourced project working with real world data was desired
- Other projects can be found scraping DS/Analytics jobs from Indeed. Typically word frequencies for keywords like Python or Hadoop are calculated
- Moving beyond that, we were interested in clustering and how the choice of words signals relationships between roles, as well as how skills relate, not just their frequency
- Job postings fit the 'bag of words' or ngram approach taught in class. Not many employers say *"We don't want someone who knows Python"*

```{r Import Libraries}
library(feather)
library(tidyverse)
library(tidytext)
library(tm)
library(wordcloud)
library(widyr)
library(ggraph)
library(igraph)
library(knitr)
library(ggridges)
library(RTextTools)
library(dendextend)
library(ggdendro)
library(clValid)
library(topicmodels)
```

##Gathering Data
- Beautiful Soup & Selenium were used in Python to access [Indeed](https://www.indeed.ca/jobs?q=analytics&l=Toronto&start=10 "Indeed:Analytics Jobs in Toronto") and scrape unsponsored job titles, companies, and postings
- 1800 jobs were scraped from 9 search terms we believed captured the jobs most MMA students are pursuing.
- Jobs were passed from Python to R using [Feather](https://blog.rstudio.com/2016/03/29/feather/ "Feather: A Fast On-Disk Format for Data Frames for R and Python, powered by Apache Arrow")

```{r Import Data}
rm(list=ls())

#list our data files
searches <- c("analytics",
                 "data analyst",
                 "data scientist",
                 "analytics strategy",
                 "data insights",
                 "marketing analytics",
                 "analytics reporting",
                 "machine learning",
                 "business intelligence")

files <- paste("data/feather/",searches,".feather",sep="")

#read and collapse to data frame
datalist <- lapply(as.list(files),function(x){read_feather(x)})
test <- datalist[[1]]
data <- bind_rows(datalist,.id="search")
rm(datalist)

#fix quotations in column names
names(data) <- c("search","company","text","titles","urls")
data <- data %>% select(company,titles,text,search,urls)

#examine the uniqueness of our data
NumJobs <- n_distinct(data$urls)

#reduce to distinct jobs and clean up search column
data <- data[!duplicated(data$urls),]
data$search <- plyr::mapvalues(data$search,
                               from=unique(data$search),
                               to=searches)
```

- Our data returned `r NumJobs` unique jobs within our search.
- Considerable data cleaning is required to get to something easy to analyze. This includes stripping remaining HTML from our text, removing custom low value words, and words too common in job postings.

##Exploratory Data Analysis
```{r preprocessing}
RemovePattern <- function(vector,pattern){gsub(pattern=pattern,replacement=" ",vector)}

#remove regex patterns of html junk, and numbers which complicate our bigrams.
data <- dmap(data,RemovePattern,"\n")
data <- dmap(data,RemovePattern,"\\(")   
data <- dmap(data,RemovePattern,"\\)")
data <- dmap(data,RemovePattern,"\\{[a-zA-Z0-9]\\}")
data$text <- RemovePattern(data$text,"[[:digit:]]")
```

```{r ggplot theme}
#to make defaults better for slide deck 
presentation <- theme_minimal()+
     theme(axis.text.x = element_text(size=12),
                      axis.text.y = element_text(size=12),
                      axis.title.x = element_text(size=14),
                      axis.title.y = element_text(size=14),
                      plot.title = element_text(size=16))

queens_colors <- scale_fill_manual(values=c("#9d1939","#11335d","#eebd31"))

```

```{r unique postings by search}
#investigate redundant jobs. Should return 200/each if they are all unique.

data %>%
     group_by(search) %>%
     summarize(NumberUniquePostings=n()) %>%
     ggplot(aes(x=factor(search,searches),y=NumberUniquePostings,fill="#11335d"))+geom_col() +
     labs(y="Number of Unseen Postings",
          title="Unique Postings by Sequential Search",
          x="Job Title") +
     presentation+
     queens_colors+
     theme(axis.text.x = element_text(angle = 30, hjust = 1))+
     theme(legend.position="none")
```

- We expect 200 jobs for each result, and removing the duplicate jobs in the order they were searched.
- Interestingly, searching 200 jobs in analytics returns only about half unique jobs, so by the time you reach page 10, you are seeing very little new things.
- As we search overlapping terms, data sciencist, data insights, fewer and fewer unique jobs are returned
- Interestingly, each additional search term returns a surprising amount of new jobs. A reasonable amount are shown for machine learning that were not found for data scientist or analytics, an overlapping field.
- Business Intelligence and marketing analytics seems to be orthogonal to other search terms, returning relatively more unique jobs

```{r most frequent titles}
#get something a list to organize our axis labels by
titles_to_use <- data %>%
     count(titles,sort=TRUE) %>%
     head(7) %>%
     mutate(titles=factor(titles,titles))

# generate a dataset with ordered search terms and titles
data %>% 
     filter(titles %in% titles_to_use$titles) %>%
     count(titles,search,sort=TRUE) %>%
     mutate(frequency = n/NumJobs) %>%
     ungroup() %>%
     mutate(titles=factor(titles,titles_to_use$titles)) %>%
     mutate(search=factor(search,searches)) %>%
     ggplot(aes(x=titles,y=frequency,fill=search))+geom_col() + 
     scale_y_continuous(name="Frequency in Dataset",labels= scales::percent) + 
     labs(title="Most Frequent Job Titles",x="Job Title") + coord_flip() + 
     presentation+scale_fill_brewer(palette="Reds",direction=-1)
```

- The job landscape is currently dominated by data scientists, which have become a catch all word. But it's encouraging to see machine learning engineers and developer roles begin to be fleshed out.
- Analytics is surprisingly absent, but is likely wrapped into titles like "Manager, Analytics" which is more inconsistently titled. Let's take a closer look at where our Analytics jobs are.

```{r most frequent analytics titles}
analytics_jobs <- data %>% filter(search=="analytics") %>% n_distinct()

data %>% 
     filter(search=="analytics") %>%
     count(titles,sort=TRUE) %>%
     head(7) %>%
     mutate(frequency = n/analytics_jobs) %>%
     ggplot(aes(x=reorder(titles,-frequency),y=frequency,fill="red"))+geom_col() + 
     theme(axis.text.x = element_text(angle = 30, hjust = 1)) + 
     scale_y_continuous(name="Frequency in Analytics Search",labels= scales::percent) +
     labs(title="Most Frequent Hiring Companies for Analytics",x="Company") + 
     coord_flip() +
     theme_minimal() + 
     theme(legend.position="none") + queens_colors
```

- These searches appear less consistent than job titles like Data Scientist.

```{r most frequent companies}
data %>% 
     count(company,sort=TRUE) %>%
     mutate(frequency = n/NumJobs) %>%  
     mutate(company = factor(company,company)) %>%
     top_n(10) %>%
     ggplot(aes(x=company,y=frequency,fill="red"))+geom_col() +
     presentation+
     queens_colors+
     theme(axis.text.x = element_text(angle = 30, hjust = 1))+
     scale_y_continuous(name="Frequency in Dataset",labels= scales::percent) + 
     labs(title="Most Frequent Hiring Companies",x="Company")+
     theme(legend.position="none")

```

- This seems to resonate with what the Toronto Job environment is as a whole: Consulting, Banking, Telecom and a splattering of retail.

##A Word Frequency Approach
```{r remove outliers}
#remove jobs where the scraper failed, that's nearly always <300 words.
MinWords <- 300

empty_urls <- data %>%
     unnest_tokens(token="words",output="unigrams",input=text) %>%
     group_by(urls) %>%
     count(urls,sort=TRUE) %>%
     filter(n < MinWords)

data <- data %>% 
     filter(!urls %in% empty_urls$urls)

NumJobs <- n_distinct(data$urls)
     
```

- The boiler plate at the end of each job posting, encouraging people to apply, discussing company acolades and culture distort our analysis. Let's spend some time cleaning up *job specific words* and *html related language*

```{r unigrams count}
#what words to avoid
stop <- read.csv("stopwords.csv",stringsAsFactors = FALSE)
stop <- rbind(stop,
              data.frame(words=stopwords("en")))

#process n-grams
data %>%
     unnest_tokens(token="words",output="unigrams",input=text) %>%
     group_by(search,unigrams) %>%
     filter(!unigrams %in% stop$words) %>%
     count(search,unigrams,sort=TRUE) %>%
     top_n(4) %>%
     ungroup() %>%
     ggplot(aes(x=unigrams,y=n,fill=search))+
     geom_bar(stat="identity")+
     coord_flip()+
     presentation+scale_fill_brewer(palette="Reds",direction=-1)+
     labs(y="Frequency of word",x="Unigram",title="Most Frequently Mentioned Unigrams by Search")
```

- We've removed most of the job specific language, apply, description and words that don't signal much about what the job is.  We see from a frequency approach, there isn't alot to be gleaned.
- Some words are mentioned in every posting. Analytics as a search term appeared to have proportionally more management oriented positions.
- Let's see if our bi-grams have more signal.
```{r bigrams count}
#look a bi-grams
data %>%
     unnest_tokens(token="ngrams",n=2,output="tokens",input=text) %>%
     group_by(search,tokens) %>%
     filter(!tokens %in% searches) %>%
     separate(col=tokens,into=c("word1","word2"),sep=" ") %>%
     filter(!word1 %in% stop$words, !word2 %in% stop$words) %>%
     unite(tokens,word1,word2,sep=" ") %>%
     filter(!tokens %in% searches) %>%
     count(search,tokens,sort=TRUE) %>%
     top_n(2) %>%
     ungroup() %>%
     ggplot(aes(x=tokens,y=n,fill=search))+
     geom_bar(stat="identity")+
     coord_flip()+
     presentation+scale_fill_brewer(palette="Reds",direction=-1)+
     labs(x="Number of Mentions",
          y="Bigram",
          title="Most Frequently Mentioned Bigrams by Search")
```

- This is more encouraging than our Unigrams. We have some domain specific phrases, like mental health and real estate. But also *communication skills* and *problem solving* which straddle the hard and soft skills often critical to success in analytics and data science.
- Some of these phrases may be loaded in a small number of job postings. For example, *digital marketing* being mentioned many times in 1 posting referring to the job title, department, and responsibilities. Let's remove phrases mentioned more than once and see more of the breadth of mentions.

```{r distinct bigrams count}
#look a bi-grams
data %>%
     unnest_tokens(token="ngrams",n=2,output="tokens",input=text) %>%
     group_by(search,tokens) %>%
     filter(!tokens %in% searches) %>%
     separate(col=tokens,into=c("word1","word2"),sep=" ") %>%
     filter(!word1 %in% stop$words, !word2 %in% stop$words) %>%
     unite(tokens,word1,word2,sep=" ") %>%
     ungroup() %>%
     distinct() %>%
     count(search,tokens,sort=TRUE) %>%
     top_n(3) %>%
     ggplot(aes(x=tokens,y=n,fill=search))+
     geom_bar(stat="identity")+
     coord_flip()+
     presentation+scale_fill_brewer(palette="Reds",direction=-1)+
     labs(x="Bigrams",y="Frequency",title="Frequency of Bigrams by Distinct Phrases")

```

- This begins to get a bit more accurate of a assessment of what employers mention. Some of these highlight more useful skills that were drowned out by more freqent mentions. These are things like *project management* or *software engineering*, useful skills for data scientists and analysts.

#A Skills Based Approach
- Typically when you see projects like this done, people look for some Analytics or Data Science skills, and count the occurences.  We want to go beyond that, but lets examine the landscape for analytical skills in Toronto.
```{r skills mentioned}

skills <- read_csv("skills.csv")
names(skills) <- c("tokens","domain")

data %>%
     unnest_tokens(token="words",output="tokens",input=text) %>%
     filter(tokens %in% skills$tokens) %>%
     group_by(tokens) %>%
     count(tokens,sort=TRUE) %>%
     top_n(12) %>%
     left_join(skills) %>%
     mutate(tokens=factor(tokens,tokens)) %>%
     ggplot(aes(x=tokens,y=n,fill=domain)) +
     geom_col() + 
     labs(title="Skills Present in Dataset",x="Skill",y="Number of Occurences in Dataset")+
     presentation+
     theme(axis.text.x = element_text(angle = 30, hjust = 1))+
     queens_colors
     

     
```

- Our list is a few dozen unigram skills that we believe capture the technologies worked in across analytics and data science. Broadly they'll get classified as Big Data, Data Analysis and Visualization to capture the analysis and communication of results, as well as the unique tools for cloud & distributed computing.
- This seems to suggest excel, R and SQL are in high demand. Let's examine how inter related these concepts are.
- Are the same jobs looking for R excel and SQL?
- How many of these skills are required for different jobs?

```{r histogram of skills}

data %>%
     unnest_tokens(token="words",output="tokens",input=text) %>%
     filter(tokens %in% skills$tokens) %>%
     select(search,tokens,urls) %>%
     distinct() %>%
     group_by(search,urls) %>%
     count(urls,sort=TRUE) %>%
     ggplot(aes(x=n,y=search,fill=search))+geom_density_ridges()+
     labs(title="Mentions of Skills in Job Postings",x="Number of Skills",y="Search Term")+
     scale_x_continuous(limits=c(0,8)) +
     presentation+
     scale_fill_cyclical(values=c("#11335d","#9d1939","#eebd31"))
```

- For the skills we have selected, analytics and data scientists have long tails. These are likely associated with the similarity between the big data tools we selected: hive, scala, spark etc, but also suggest companies are casting a wide net in terms of people's experience.
- For the words we selected, many jobs in marketing analysis and business intelligence don't seem to leverage them as much as other positions.
- Let's see how theses skills get mentioned together. 

# A Network Diagram of Skills
```{r pairwise correlation}
#pairwise correlation
data %>%
     unnest_tokens(token="words",output="tokens",input=text) %>%
     filter(tokens %in% skills$tokens) %>%
     pairwise_cor(tokens,urls,sort=TRUE) %>%
     filter(correlation > .18) %>%
     graph_from_data_frame() %>%
     ggraph(layout = "fr") +
     geom_edge_link(aes(#edge_width=correlation,
                         edge_alpha = correlation),show.legend = FALSE) +
     geom_node_point(color = "#eebd31", size = 8) +
     geom_node_text(aes(label = name), repel = TRUE) +
     theme_void()

```

- The network analysis shown shows a few interesting groupings with darker lines representing more frequently correlated words. A line between two words representing a likelihood to be mentioned together in the same job.
- Excel and powerpoint don't seem correlated with the rest of our tech stack, despite the frequent mentions of excel (which presumably are the noun and not the verb)
- Traditional Analytics - R, SAS, and SPSS seem inter-related.
- Big Data - Python leveraging Hadoop, AWS, Scala and spark. Interestingly R is not the language of big data despite some support from spark.
- BI/Data Viz - Tableau, microstrategy and qlik supported by SQL.
- The most freqent words, R, SQL, and excel no longer seem as inter-related.
- Let's look at clustering our data set, to see if these groups are also represented when we cluster on all the words in the posting.


```{r clustering - Scree Plot}
# 
# #Frequency filters
# 
# minFreq = 0.05
# maxFreq = 0.80
# 
# #creating identifier
# data$ID <- paste(data$company,data$titles,sep="_")
# 
# #filtering out stopwords and infrequent/frequent words from unigrams
# clean_unigrams <- data %>%
#      unnest_tokens(token="words",output="tokens",input=text) %>%
#      select(urls,tokens) %>%
#      distinct() %>%
#      filter(!tokens %in% stop$words) %>%
#      mutate(tokens=wordStem(tokens)) %>%
#      group_by(tokens) %>%
#      count(tokens,sort=TRUE) %>%
#      mutate(frequency=n/NumJobs) %>%
#      filter(frequency> minFreq & frequency< maxFreq)
# 
# #creating clean unigrams DTM
# clustering_unigrams <- data %>%
#      unnest_tokens(token="words",output="tokens",input=text) %>%
#      select(ID,tokens) %>%
#      mutate(tokens=wordStem(tokens)) %>%
#      filter(tokens %in% clean_unigrams$tokens) %>%
#      count(ID,tokens,sort=TRUE) %>%
#      ungroup() %>%
#      spread(tokens,n,fill=0) 
# 
# #filtering out stopwords and infrequent/frequent words from bigrams
# clean_bigrams <- data %>%
#      unnest_tokens(token="ngrams",n=2,output="tokens",input=text) %>%
#      select(urls,tokens) %>%
#      distinct() %>%
#      separate(col=tokens,into=c("word1","word2"),sep=" ") %>%
#      filter(!word1 %in% stop$words, !word2 %in% stop$words) %>%
#      mutate(word1=wordStem(word1)) %>%
#      mutate(word2=wordStem(word2)) %>%
#      unite(tokens,word1,word2,sep=" ") %>%
#      group_by(tokens) %>%
#      count(tokens,sort=TRUE) %>%
#      ungroup() %>%
#      mutate(frequency=n/NumJobs) %>%
#      filter(frequency> minFreq & frequency< maxFreq)
# 
# #creating clean bigrams dataset
# clustering_bigrams <- data %>%
#      unnest_tokens(token="ngrams",n=2,output="tokens",input=text) %>%
#      select(ID,tokens) %>%
#      separate(col=tokens,into=c("word1","word2"),sep=" ") %>%
#      mutate(word1=wordStem(word1)) %>%
#      mutate(word2=wordStem(word2)) %>%
#      unite(tokens,word1,word2,sep=" ") %>%
#      filter(tokens %in% clean_bigrams$tokens) %>% 
#      count(ID,tokens,sort=TRUE) %>%
#      ungroup() %>%
#      spread(tokens,n,fill=0) 
# 
# #Joining unigrams and bigrams cluster data together, replace NA's
# clustering_data <-  clustering_unigrams %>%
#      full_join(clustering_bigrams) %>%
#      slice(-c(58,111,241,328,435,529))
# 
# #collapse to numeric
# clustering_data[is.na(clustering_data)] <- 0
# clusteringID <- clustering_data$ID
# clustering_data <- select(clustering_data,-ID)
#      
#```
# 
# - An initial pass using hierarchical clustering revealed a half dozen outlier jobs, which were removed, the dendrogram will be omitted due to it's size and for the sake of brevity.
# - Let's instead see how K-means clustering performs, this being a semi-supervised problem. We would expect some of the search terms to load together in the same cluster if they are similar jobs. Perhaps Data Scientist and Machine learning in 1 cluster, with marketing analytics in another.
# 
# ```{r 30 clusters}
# #accumulator of clustering results
# clust_results <- data.frame()
# 
# #run kmeans for all clusters up to 30
# for(i in 1:30) {
#      k_clust <- kmeans(clustering_data, centers=i, iter.max =100)
#      #Combine cluster number and cluster together, write to df
#      clust_results <- rbind(clust_results,cbind(i,k_clust$tot.withinss))
# }
# names(clust_results) <- c("cluster","results")
# 
# #scree elbow plot
# ggplot(clust_results,aes(x=cluster,y=results))+
# geom_point(col="#9d1939",size=4)+geom_line(col="#9d1939",size=2)+
#      presentation +
#      labs(title="Scree Plot - Within Cluster Variance vs Number of clusters",
#           x="Number of Clusters",
#           y="Within Cluster Sum of Squares")
# 
# #performance metrics
# validation_metrics <- clValid(as.matrix(clustering_data),3:10,clMethods="kmeans",
#                               validation="internal")

# 
# - Plotting the within cluster sum of squares vs number of clusters produces a scree plot. Here, good clustering would be judged by a sharp "elbow" in the data. We don't see that here.
# - Evaluating instead by Dunn's Metric, which judgues clusters by the means of clusters, the distance between clusters and the within cluster variance.  Here, we find the ideal cluster size to be 7. Let's dive a litle further into our clustering results.

# ```{r 7 cluster performance, results=FALSE}
# kmeans7 <- kmeans(clustering_data,centers=7,nstart=100)
# cluster7results <- data.frame(ID=as.character(clusteringID),clusters=kmeans7$cluster)
# data <- data %>%
#      mutate(ID=paste(company,titles,sep="_"))
# 
# left_join(cluster7results,data,by="ID") %>%
#      select(search,clusters) %>%
#      count(search,clusters,sort=TRUE) %>%
#      ggplot(aes(x=search,y=clusters,fill=n))+
#      geom_tile()+
#      geom_text(aes(label=n))+
#      presentation+
#      scale_fill_gradient(low="#11335d",high="#9d1939")+
#      theme(axis.text.x = element_text(angle = 30, hjust = 1),
#            plot.title = element_text(hjust = 0.5)) + 
#      labs(title="Density of Cluster Assignment vs Initial Search Term",
#           x="Search Term",
#           y="Cluster Number")
     


# - Words were stemmed and unigrams and bigrams that occur in between 10% and 80% of postings were used.
# - In reality, these 7 clusters are really just 3. Most of our jobs are loading in clusters 4,5 and 6.
# - Even these clusters don't seem to represent sensible structure, cluster 4 has jobs in data science, data analyst and marketing analytics highly loaded, which don't seem interrelated at first glance.
# - Clusters 1,2,3 and 7 are just outliers, and don't seem to measure anything.
# - K-means is sensitive to multi-dimensional outliers, which are hard to identify. With more work identifiying them and filtering them out, we could achieve more resolution between our clusters. But wasn't achievable in 4 weeks.
```
<!-- ##Topic Modelling -->
<!-- - We should exepct that this data set is well suited for topic modelling. -->
<!-- - It likely consists of themes within the job postings, we saw earlier with different tech stacks, but also with an emphasis on soft skills. -->
<!-- - We might also see topics loaded with other aspects of the business, industry specific knowledge like credit modelling in finance, or churn and lift models in marketting. -->

```{r}
# # exploring topic modelling....
# LDA_bigrams <- data %>%
#      unnest_tokens(token="ngrams",n=2,output="tokens",input=text) %>%
#      select(ID,tokens) %>%
#      separate(col=tokens,into=c("word1","word2"),sep=" ") %>%
#      mutate(word1=wordStem(word1)) %>%
#      mutate(word2=wordStem(word2)) %>%
#      unite(tokens,word1,word2,sep=" ") %>%
#      filter(tokens %in% clean_bigrams$tokens) %>% 
#      count(ID,tokens,sort=TRUE)
# 
# LDA_unigrams <- data %>%
#      unnest_tokens(token="words",output="tokens",input=text) %>%
#      select(ID,tokens) %>%
#      mutate(tokens=wordStem(tokens)) %>%
#      filter(tokens %in% clean_unigrams$tokens) %>%
#      count(ID,tokens,sort=TRUE)
# 
# lda_data <- bind_rows(LDA_unigrams,LDA_bigrams)
# names(lda_data)
# lda_dtm <- cast_dtm(lda_data,
#                     term=tokens,
#                     document=ID,
#                     value=n)
# 
# 
# lda <- LDA(lda_dtm, k = 3, control = list(seed = 42))
# test <- tidy(lda,matrix="beta")
# test %>%
#      group_by(topic) %>%
#      ungroup() %>%
#      arrange(topic,-beta) %>%
#      filter(term %in% skills$tokens) %>%
#      spread(topic,beta)
# 
# test2 <- tidy(lda,matrix="gamma")
# 
# kable(test2 %>%
#      group_by(topic) %>%
#      top_n(20,gamma))
# 

```

#Conclusion

- While employers demand a variety of technical skills, it's notable that softer skills are also important. The role of analytics in an organization is not only to generate insight but also to communicate it.
- R, SQL and Excel are most demanded tools in Toronto, but not in the same roles. 
- Distinct groupings could be seen for skillsets in conventional analytics tools, data visualization & dashboarding, and the big data tech stack.

