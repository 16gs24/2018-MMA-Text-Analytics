---
title: "The Toronto Analytics Job Landscape"
author: "Summerhill"
date: '`r Sys.Date()`'
output:
  pdf_document: default
  md_document:
    variant: markdown_github
  html_document: default
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.width=10, fig.height=10, fig.path='Figs/',
                      warning=FALSE,message=FALSE,error=FALSE,include=TRUE,echo=TRUE)

set.seed(42)

# a note on options: 
# include=TRUE means that output of the code is shown in the document (intended for graphics).
# echo=TRUE means the code is shown in the document (intended for code that might be interesting for a reader).
# message and warning are for the text for loading libraries or if a function fails
# echo=TRUE means the code is not shown in the final document
# http://kbroman.org/knitr_knutshell/pages/Rmarkdown.html
# http://rmarkdown.rstudio.com/developer_parameterized_reports.html
# https://guides.github.com/features/mastering-markdown/

```
- This document was rendered last on `r Sys.Date()`

##*THIS PROJECT IS STILL UNDER CONSTRUCTION*
The intention will be to mask the code as the project approaches completion.

##Executive Summary
- This project was produced for the Text Analytics Workshop for the Winter 2018 Masters of Management Analytics Cohort at Queen's University
- The goal from the outset was to use text analytics techniques developed in class to examine jobs companies have posted on Indeed in Toronto
and employ techniques discussed in class including document clustering, topic modelling, and visualization.

##Project Rationale
- A open sourced project working with real world data was desired
- Other projects can be found scraping DS/Analytics jobs from Indeed. Typically word frequencies for keywords like Python or Hadoop are calculated
- Moving beyond that, we were interested in topic modelling and how the choice of words signals relationships between roles
- Job postings fit the 'bag of words' or ngram approach taught in class. Not many employers say **"We don't want someone who knows Python"**

```{r Import Libraries}
library(feather)
library(tidyverse)
library(tidytext)
library(tm)
library(wordcloud)
library(widyr)
library(ggraph)
library(igraph)
library(knitr)
library(ggridges)
library(purrrlyr)
```

##Gathering Data
- Beautiful Soup & Selenium were used in Python to access [Indeed](https://www.indeed.ca/jobs?q=analytics&l=Toronto&start=10 "Indeed:Analytics Jobs in Toronto") and scrape unsponsored job titles, companies, and postings
- `later number` unique jobs were scraped from the search terms: `analytics`,`etc`....
- Jobs were passed from Python to R using [Feather](https://blog.rstudio.com/2016/03/29/feather/ "Feather: A Fast On-Disk Format for Data Frames for R and Python, powered by Apache Arrow")

```{r Import Data,echo=TRUE}
rm(list=ls())
#list our data files
searches <- c("analytics",
                 "data analyst",
                 "data scientist",
                 "analytics strategy",
                 "data insights",
                 "marketing analytics",
                 "analytics reporting",
                 "machine learning",
                 "business intelligence")

files <- paste("data/feather/",searches,".feather",sep="")

#read and collapse to data frame
datalist <- lapply(as.list(files),function(x){read_feather(x)})
test <- datalist[[1]]
data <- bind_rows(datalist,.id="search")
rm(datalist)

#fix quotations in column names
names(data) <- c("search","company","text","titles","urls")
data <- data %>% select(company,titles,text,search,urls)

#check if we have redundant jobs
sum(duplicated(data[,2:4]))

#examine the uniqueness of our data
NumJobs <- n_distinct(data$urls)

#reduce to distinct jobs and clean up search column
data <- data[!duplicated(data$urls),]
data$search <- plyr::mapvalues(data$search,
                               from=unique(data$search),
                               to=searches)
```
- Our data returned `NumJobs` unique jobs within our search.
- It's clear a considerable amount of cleaning is in order 

```{r preprocessing}
RemovePattern <- function(vector,pattern){gsub(pattern=pattern,replacement=" ",vector)}
data <- dmap(data,RemovePattern,"\n")
data <- dmap(data,RemovePattern,"\\(")   
data <- dmap(data,RemovePattern,"\\)")
data <- dmap(data,RemovePattern,"\\{[a-zA-Z0-9]\\}")
```

```{r Jobs Found}
#investigate redundant jobs. Should return 200/each if they are all unique.

rollup <- data %>%
     group_by(search) %>%
     summarize(NumberUniquePostings=n())

str(rollup)

#sort by search order
kable(left_join(data.frame(search=searches),rollup,by="search"))
```

- We expect 200 jobs for each result, and removing the duplicate jobs in the order they were searched.
- Interestingly, searching 200 jobs in analytics returns only 113 unique jobs, some redundancy exists.
- As we search overlapping terms, data sciencist, data insights, fewer and fewer unique jobs are returned
- Interestingly, each additional search term returns a surprising amount of new jobs. 75 jobs are shown for machine learning that were not found for data scientist, a fairly similar field.
- Business Intelligence seems to be fairly lateral to other search terms, returning many unique jobs

```{r}
kable(head(data %>% group_by(search,titles) %>% count(sort=TRUE),10))
```
```{r}
kable(head(data %>% count(company,sort=TRUE),10))
```

```{r Remove empty jobs}
#how long are our jobs
data %>%
     unnest_tokens(token="words",output="unigrams",input=text) %>%
     group_by(urls) %>%
     count(search,urls,sort=TRUE) %>%
     filter(n<3000) %>%
     ggplot(aes(x=n,y=search))+geom_density_ridges()
```

- We see that there are alot of 0 information jobs, postings with only a few words, let's look at what some of those are.

```{r remove outliers}
MinWords <- 300

empty_urls <- data %>%
     unnest_tokens(token="words",output="unigrams",input=text) %>%
     group_by(urls) %>%
     count(urls,sort=TRUE) %>%
     filter(n < MinWords)

# data %>% 
#      filter(urls %in% empty_urls$urls) %>%
#      head()

data <- data %>% 
     filter(!urls %in% empty_urls$urls)
     
```

- We see here a variety of failed scrapings, or 0 info postings


```{r long postings}
long_postings <- data %>%
     unnest_tokens(token="words",output="unigrams",input=text) %>%
     group_by(urls) %>%
     count(urls,sort=TRUE) %>%
     filter(n >3000)

```

```{r}
unigrams <- data %>%
     unnest_tokens(token="words",output="unigrams",input=text) %>%
     group_by(unigrams) %>%
     count(unigrams,sort=TRUE)

wordcloud(unigrams$unigrams,unigrams$n,max.words=50)
```


- Looking at a simple word frequency, we see out of the box our data is very messy
- The boiler plate at the end of each job posting, encouraging people to apply, discussing company acolades and culture distort our analysis. Let's spend some time cleaning up *common stopwords*, *job specific words* and *html*

```{r Process unigrams Data}
#what words to avoid
stop <- read.csv("stopwords.csv",stringsAsFactors = FALSE)
stop <- rbind(stop,data.frame(words=stopwords("en")))


#process n-grams
unigrams <- data %>%
     unnest_tokens(token="words",output="unigrams",input=text) %>%
     group_by(unigrams) %>%
     filter(!unigrams %in% stop$words) %>%
     count(unigrams,sort=TRUE)

#visualize
wordcloud(unigrams$unigrams,unigrams$n,max.words=50)
```

- We are starting to look better. Let's take a look at our bigrams.
```{r Process bigrams}
#look a bi-grams
bigrams_totals <- data %>%
     unnest_tokens(token="ngrams",n=2,output="tokens",input=text) %>%
     separate(col=tokens,into=c("word1","word2"),sep=" ") %>%
     filter(!word1 %in% stop$words, !word2 %in% stop$words) %>%
     unite(tokens,word1,word2,sep=" ") %>%
     count(tokens,sort=TRUE)

#kable(head(bigrams_totals,20))

wordcloud(bigrams_totals$tokens,bigrams_totals$n,max.words=10)
```



```{r Unigram frequency}

#determine how frequent each word occurs across job postings. Don't skip stop words yet.
unigrams_freq <- data %>%
     unnest_tokens(token="words",output="tokens",input=text) %>%
     select(urls,tokens) %>%
     distinct() %>%
     filter(!tokens %in% stop$words) %>%
     group_by(tokens) %>%
     count(tokens,sort=TRUE) %>%
     ungroup() %>%
     mutate(frequency=n/NumJobs)

kable(head(unigrams_freq,20))
```


```{r}

#determine how frequent each bigram is across job postings. Don't skip stop words yet.
bigrams_freq <- data %>%
     unnest_tokens(token="ngrams",n=2,output="tokens",input=text) %>%
     select(urls,tokens) %>%
     distinct() %>%
     separate(col=tokens,into=c("word1","word2"),sep=" ") %>%
     filter(!word1 %in% stop$words, !word2 %in% stop$words) %>%
     unite(tokens,word1,word2,sep=" ") %>%
     group_by(tokens) %>%
     count(tokens,sort=TRUE) %>%
     ungroup() %>%
     mutate(frequency=n/NumJobs)

kable(head(bigrams_freq,20))

```



```{r pairwise count}

skills <- c("sas","python","sql","r","spss",
            "hadoop","spark","java","scala","aws","c++",
            "excel","powerpoint","mapreduce",
          "qlik","tableau","microstrategy","azure")

data %>%
     unnest_tokens(token="words",output="tokens",input=text) %>%
     filter(tokens %in% skills) %>%
     pairwise_count(tokens,urls,sort=TRUE) %>%
     head()

data %>%
     unnest_tokens(token="words",output="tokens",input=text) %>%
     filter(tokens %in% skills) %>%
     ggplot(aes(x=tokens)) +
     geom_bar()
```





```{r pairwise correlation}
#pairwise correlation
data %>%
     unnest_tokens(token="words",output="tokens",input=text) %>%
     filter(tokens %in% skills) %>%
     pairwise_cor(tokens,urls,sort=TRUE) %>%
     filter(correlation > .2) %>%
     graph_from_data_frame() %>%
     ggraph(layout = "fr") +
     geom_edge_link(aes(edge_alpha = correlation), show.legend = FALSE) +
     geom_node_point(color = "lightblue", size = 5) +
     geom_node_text(aes(label = name), repel = TRUE) +
     theme_void()

```

```{r clustering - Scree Plot}

#Frequency filters

minFreq = 0.10
maxFreq = 0.80

#filtering out stopwords and infrequent/frequent words from unigrams
unigram_clean <- data %>%
     unnest_tokens(token="words",output="tokens",input=text) %>%
     select(urls,tokens) %>%
     distinct() %>%
     filter(!tokens %in% stop$words) %>%
     group_by(tokens) %>%
     count(tokens,sort=TRUE) %>%
     mutate(frequency=n/NumJobs) %>%
     filter(frequency> minFreq & frequency< maxFreq)

#creating identifier
data$ID <- paste(data$company,data$titles,sep="_")

#creating clean unigrams DTM
unigram_cluster <- data %>%
     unnest_tokens(token="words",output="tokens",input=text) %>%
     select(ID,tokens) %>%
     filter(tokens %in% unigram_clean$tokens) %>% 
     count(ID,tokens,sort=TRUE) %>%
     ungroup() %>%
     spread(tokens,n,fill=0) 

#filtering out stopwords and infrequent/frequent words from bigrams
bigram_clean <- data %>%
     unnest_tokens(token="ngrams",n=2,output="tokens",input=text) %>%
     select(urls,tokens) %>%
     distinct() %>%
     separate(col=tokens,into=c("word1","word2"),sep=" ") %>%
     filter(!word1 %in% stop$words, !word2 %in% stop$words) %>%
     unite(tokens,word1,word2,sep=" ") %>%
     group_by(tokens) %>%
     count(tokens,sort=TRUE) %>%
     ungroup() %>%
     mutate(frequency=n/NumJobs) %>%
     filter(frequency> minFreq & frequency< maxFreq)

#creating clean bigrams dataset
bigram_cluster <- data %>%
     unnest_tokens(token="ngrams",n=2,output="tokens",input=text) %>%
     select(ID,tokens) %>%
     filter(tokens %in% bigram_clean$tokens) %>% 
     count(ID,tokens,sort=TRUE) %>%
     ungroup() %>%
     spread(tokens,n,fill=0) 

#Joining unigrams and bigrams cluster data together 

data_cluster <- left_join(unigram_cluster,bigram_cluster)

#convert to dataframe
data_cluster <- as.data.frame(data_cluster)


#accumulator of clustering results
clust_results <- data.frame()

#run kmeans for all clusters up to 30
set.seed(100)
for(i in 1:30) {
     k_clust <- kmeans(data_cluster[,-1], centers=i, iter.max =100)
     #Combine cluster number and cluster together, write to df
     clust_results <- rbind(clust_results,cbind(i,k_clust$tot.withinss))
}
names(clust_results) <- c("cluster","results")

#scree elbow plot
ggplot(clust_results,aes(x=cluster,y=results,group=1))+
geom_point()+geom_line()
```
The scree elbow plot above does not really have an elbow but seems like at around the 10th cluster, the drop in within sum-of-squares becomes considerably lower for each additional cluster.    

```{r}

#convert features in dataframe from character to numeric class
data_cluster[,2:785] <- sapply(data_cluster[,2:785],as.numeric)


# #need to create principal components, regular kmeans will kick out an error because it uses princomp function
pc <- prcomp(data_cluster[,-1])
plot(pc,xlab="Principal Components")
summary(pc)
comp <- data.frame(pc$x[,1:15])
 

```


```{r clustering - kmeans}
#Let's look at 3 clusters
kmeans2 <- kmeans(data_cluster[,-1],centers=2)
kmeans3 <- kmeans(data_cluster[,-1],centers=3)
kmeans6 <- kmeans(data_cluster[,-1],centers=6)
kmeans10 <- kmeans(data_cluster[,-1],centers=10)

#Plot clusters
library(cluster)
kmeans2_plot <- clusplot(comp, kmeans2$cluster, color=TRUE, shade=FALSE,
         labels=2, lines=0, main="K-means cluster plot")
kmeans3_plot <- clusplot(comp, kmeans3$cluster, color=TRUE, shade=FALSE,
         labels=3, lines=0, main="K-means cluster plot") #3 clusters look the best
kmeans6_plot <- clusplot(comp, kmeans6$cluster, color=TRUE, shade=FALSE,
         labels=2, lines=0, main="K-means cluster plot")
kmeans10_plot <- clusplot(comp, kmeans10$cluster, color=TRUE, shade=FALSE,
         labels=2, lines=0, main="K-means cluster plot")
```

Common outliers for these jobs include job #58, #184, #331, #111, #78 and maybe #73. Urls for these jobs: 

#58: CNR Data Scientist: http://jobs.cn.ca/jobs/8564BR/Canada-Qu%C3%A9bec-Montreal--Data-Scientist?codes=1-INDEED#.WeVfnGhSxPY

Note: not quite sure why this role is an outlier. This was also an outlier in the dendrogram

#184: Johnson & Johnson Logistics Strategy Analyst: https://jobs.jnj.com/jobs/1551170927/Logistics-Strategy-Analyst-Consumer-Business?lang=en-us&src=JB-10281

Note: this role seems to be very broad. Not only would you have to deal with supply chain responsibilities but also a bit of reporting, financial analysis, project management, business process improvement, etc. Also not a lot of typical "analytics" qualifications listed other than Tableau. This was also an outlier in the dendrogram

#381: Council of Ontario Universities Data Analyst: https://charityvillage.com/jobs/search-results/job-detail.aspx?id=371533&l=2

Note: Not quite sure why this is an outlier either. Maybe because of all the talk about policy? Though in the dendrogram, this was not an outlier.

#111: Deloitte Data Scientist: https://jobs2.deloitte.com/us/en/job/DELOA004X154608/Data-Scientist?src=JB-16801

Note: Interesting that this role popped up as an outlier here and in the dendrogram considering its just a Data Scientist role. Just a quickly comparing this with the KPMG and PwC data scientist roles, this role seems like it has a lot more technical requirements than the others. The key difference that jumped out at me was that this one indicated more project management responsibilities than the other one. ie. "acting as the Scrum Master through project efforts" If we have more time, would be interesting to look into this a little more. 

#437:Lakeside Controls BI Analyst: https://www.indeed.ca/viewjob?jk=e8005300fca06421&q=lakeside+controls+business+intelligence+analyst&tk=1bsk4bn1p0k6913r&from=web

#243: PBS Sytems Juniot BI Analyst: https://pbs-systems.automotohr.com/job_details/k5w8lqu6dkh8u8dkvl3402

Note: Note quite sure about these ones, also popped up in the dendrogram. Could be because the description is a little light on the "analytics" language. 




```{r clustering - hierarchical}
library(dendextend)
library(ggdendro)
rownames(data_cluster) <- data_cluster$ID

#calculate distance
data_dist <- dist(data_cluster[,-1])

#creating and plotting hierarchical cluster
hc <- hclust(data_dist)
hcd <- as.dendrogram(hc)

hcd %>% set("branches_k_color",k=10) %>%
     plot(main="Hierarchical Clustering")

hcd_color <- set(hcd,"branches_k_color",k=10)
     
plot(cut(hcd_color,h=100)$upper,
     main="Upper tree of cut at h=100")
plot(cut(hcd_color,h=100)$lower[[2]],
     main="Second branch of lower tree with cut at h=100")


#rotating the dendrogram then printing it to PDF so you can zoom into the individual jobs
ggdendrogram(hc,rotate=TRUE)

pdf("C:/R/rotatedhcplot.pdf", width=40, height=60)
ggdendrogram(hc,rotate=TRUE)
dev.off()


```

```{r - clustering metrics}

library(clValid)

dunns_kmeans <- dunn(data_dist,kmeans3$cluster)

## validation metrics for hierarchical and kmeans. Search ?clValid for more details
validation_metrics <- clValid(data_cluster[-1],2:5,clMethods=c("hierarchical","kmeans"),validation="internal")
summary(validation_metrics)


```

```{r - assigning clusters to each job}

##turning the list of all the cluster for each job into a dataframe so it can be matched to original dataset 
kmeans_cluster_assgn <- factor(kmeans3$cluster)
kmeans_cluster_assgn <- as.data.frame(kmeans_cluster_assgn)
kmeans_cluster_assgn$ID <- rownames(kmeans_cluster_assgn)

#assigning cluster number to original dataset
data$cluster <- kmeans_cluster_assgn$kmeans_cluster_assgn[match(data$ID,kmeans_cluster_assgn$ID)]
```
